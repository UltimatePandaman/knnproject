log_dir: "Models/"
save_freq: 10
device: "cuda" # change this to debug on CPU, don't actually train this model on CPU please...
epochs: 300
batch_size: 2 # change this to utilize your GPU memory, batch of 5 takes about 10 GB
pretrained_model: "Models/epoch_00102.pth" # start at a checkpoint
load_only_params: false
fp16_run: true

train_data: "Data/train_list.txt" # prepare_train_list.ipynb places the datalists into these files
val_data: "Data/val_list.txt" # but if you have your own datalists you may change this

F0_path: "Utils/JDC/bst.t7"
ASR_config: "Utils/ASR/config.yml"
ASR_path: "Utils/ASR/epoch_00100.pth"

preprocess_params:
  sr: 24000
  spect_params:
    n_fft: 2048
    win_length: 1200
    hop_length: 300

model_params:
  dim_in: 64
  latent_dim: 16
  num_domains: 2
  max_conv_dim: 512
  n_repeat: 4
  w_hpf: 0
  F0_channel: 256

loss_params:
  g_loss:
    lambda_cyc: 5.
    lambda_norm: 1.
    lambda_asr: 10.
    lambda_f0: 5.
    lambda_adv: 2. # takže se bude generátor učit 2x rychleji než diskriminátor?
    lambda_adv_cls: 0.5 # takže se bude generátor učit 5x rychleji než klasifikátor původní domény?
    norm_bias: 0.5
  d_loss:
    lambda_reg: 1.
    lambda_adv_cls: 0.1
    lambda_con_reg: 10.
  
  adv_cls_epoch: 50
  con_reg_epoch: 30

optimizer_params:
  lr: 0.0001
